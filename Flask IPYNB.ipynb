{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbf4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask, os, io, h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.restoration import (denoise_wavelet, estimate_sigma)\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b14ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenoiseWavelet(data, type='BayesShrink'):\n",
    "    def BayesShrink():\n",
    "        im_bayes = denoise_wavelet(np.array(data), convert2ycbcr=True, multichannel=True,\n",
    "                                  method='BayesShrink', mode='soft', \n",
    "                                  rescale_sigma=True, wavelet_levels=4)\n",
    "        return im_bayes\n",
    "    \n",
    "    def VisuShrink():\n",
    "        sigma_est = estimate_sigma(np.array(data), multichannel=True, average_sigmas=True)\n",
    "        im_visu = denoise_wavelet(np.array(img), convert2ycbcr=True, multichannel=True,\n",
    "                                  method='VisuShrink', mode='soft', wavelet_levels=4,\n",
    "                                  sigma=sigma_est, rescale_sigma=True)\n",
    "        \n",
    "        return im_visu\n",
    "    \n",
    "    if type=='BayesShrink':\n",
    "        return BayesShrink()\n",
    "    elif type=='VisuShrink':\n",
    "        return VisuShrink()\n",
    "\n",
    "def create_feature(data_feature):\n",
    "    rs = []\n",
    "    data_denoise = DenoiseWavelet(data_feature, type='BayesShrink')\n",
    "    dt = tf.keras.utils.timeseries_dataset_from_array(data=data_denoise, targets=None,\n",
    "                                                          sequence_length=100, sequence_stride=20,\n",
    "                                                          shuffle=False)\n",
    "    for i in dt:\n",
    "        rs.append(i)\n",
    "        \n",
    "    rs = tf.stack(rs)\n",
    "    feature_rs = tf.data.Dataset.from_tensor_slices(rs)\n",
    "    feature_rs = feature_rs.cache().batch(56).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return feature_rs\n",
    "\n",
    "def get_predict_p(arr, threshold=.5, types='Highest'):\n",
    "    arr = arr.reshape(-1)\n",
    "    \n",
    "    if types=='Highest':\n",
    "        if np.max(arr)>=threshold:\n",
    "            return np.argmax(arr)*20+275\n",
    "        else:\n",
    "            return -1\n",
    "    elif types=='Early':\n",
    "        for i, p_prob in enumerate(arr):\n",
    "            if p_prob>=threshold:\n",
    "                return i*20+275\n",
    "        else:\n",
    "            return -1\n",
    "    elif types=='Late':\n",
    "        i_thres = None\n",
    "        for i, p_prob in enumerate(arr):\n",
    "            if p_prob>=threshold:\n",
    "                i_thres = i\n",
    "        if i_thres!=None:\n",
    "            return i_thres*20+275\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_set, batch_size=196):\n",
    "        self.x = x_set\n",
    "        self.y = np.random.random(size=(x_set.shape))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15915171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD SAVEDMODEL ===\n",
    "\n",
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = tf.keras.metrics.Precision(thresholds=0.5)\n",
    "        self.recall_fn = tf.keras.metrics.Recall(thresholds=0.5)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        self.f1.assign(2 *((p*r)/(p + r + 1e-6)))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0)\n",
    "\n",
    "PATH = './Models/P-Model'\n",
    "p_model = tf.keras.models.load_model(PATH, custom_objects={'F1_Score':F1_Score,\n",
    "                                                              'loss':tfa.losses.SigmoidFocalCrossEntropy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca93877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD SAVEDMODEL ===\n",
    "\n",
    "PATH = './Models/S-Model'\n",
    "s_model = tf.keras.models.load_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e147ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./stead_indonesia_single_wavelength.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "start_ = 35075\n",
    "end_ = start_+1200\n",
    "epochs = 352\n",
    "epochs_loop = 0\n",
    "\n",
    "is_p_detected = False\n",
    "is_s_detected = False\n",
    "\n",
    "is_p_time_detected = False\n",
    "is_s_time_detected = False\n",
    "\n",
    "temp_s, temp_mag = 0, 0\n",
    "\n",
    "p_s_mag_prediction = {}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sub_p_s_mag_prediction = {}\n",
    "\n",
    "        mult = 100\n",
    "        data_pred = create_feature(X[start_:end_, :])\n",
    "        y_pred = p_model.predict(data_pred)\n",
    "\n",
    "        high_ = get_predict_p(y_pred, types='Highest')\n",
    "        early_ = get_predict_p(y_pred, types='Early')\n",
    "        late_ = get_predict_p(y_pred, types='Late')\n",
    "\n",
    "        if (high_ and early_ and late_) != -1:\n",
    "            if is_p_time_detected==False:\n",
    "                p_time = datetime.now()\n",
    "                p_year = p_time.year\n",
    "                p_month = p_time.month\n",
    "                p_day = p_time.day\n",
    "                p_date = f'{p_day}/{p_month}/{p_year}'\n",
    "\n",
    "                p_hour = p_time.hour\n",
    "                p_minute = p_time.minute\n",
    "                p_second = p_time.second\n",
    "                p_timestamp = p_hour*3600+p_minute*60+p_second\n",
    "                is_p_time_detected = True\n",
    "\n",
    "            is_p_detected = True\n",
    "\n",
    "        if is_p_detected and ((high_ and early_ and late_) == -1):\n",
    "            data_pred_np = DenoiseWavelet(X[start_-6000:start_, :], type='BayesShrink')\n",
    "            data_pred_np = np.expand_dims(data_pred_np, axis=0)\n",
    "                        \n",
    "            data_pred = DataGenerator(data_pred_np, batch_size=196)\n",
    "            y_pred = s_model.predict(data_pred.x).flatten()\n",
    "\n",
    "            pred_s, pred_mag = y_pred[0], y_pred[1]\n",
    "            pred_s = np.exp(pred_s)\n",
    "            pred_mag = np.exp(pred_mag)-1\n",
    "                        \n",
    "            is_s_detected = True\n",
    "            \n",
    "            s_time = datetime.now()\n",
    "            s_year = s_time.year\n",
    "            s_month = s_time.month\n",
    "            s_day = s_time.day\n",
    "            s_date = f'{s_day}/{s_month}/{s_year}'\n",
    "\n",
    "            s_hour = s_time.hour\n",
    "            s_minute = s_time.minute\n",
    "            s_second = s_time.second\n",
    "            s_timestamp = s_hour*3600+s_minute*60+s_second\n",
    "                                    \n",
    "        if is_p_detected==False and is_s_detected==False:\n",
    "            sub_p_s_mag_prediction.update({\n",
    "                'P-Wave Date':-1,\n",
    "                'P-Wave TimeStamp':-1,\n",
    "                'S-Wave Time':-1,\n",
    "                'S-Wave TimeStamp':-1,\n",
    "                'Radius':-1,\n",
    "                'Latitude':-8.4702,\n",
    "                'Longitude':114.1521,\n",
    "            })\n",
    "            p_s_mag_prediction.update({epochs_loop+1:sub_p_s_mag_prediction})\n",
    "\n",
    "        elif is_p_detected==True and is_s_detected==False:\n",
    "            sub_p_s_mag_prediction.update({\n",
    "                'P-Wave Date':p_date,\n",
    "                'P-Wave TimeStamp':p_timestamp,\n",
    "                'S-Wave Date':-1,\n",
    "                'S-Wave TimeStamp':-1,\n",
    "                'Radius':-1,\n",
    "                'Latitude':-8.4702,\n",
    "                'Longitude':114.1521,\n",
    "            })\n",
    "            p_s_mag_prediction.update({epochs_loop+1:sub_p_s_mag_prediction})\n",
    "\n",
    "        elif is_p_detected==True and is_s_detected==True:\n",
    "            sub_p_s_mag_prediction.update({\n",
    "                'P-Wave Date':p_date,\n",
    "                'P-Wave TimeStamp':p_timestamp,\n",
    "                'S-Wave Date':s_date,\n",
    "                'S-Wave TimeStamp':s_timestamp,\n",
    "                'Radius':(s_timestamp-p_timestamp)*8.4,\n",
    "                'Latitude':-8.4702,\n",
    "                'Longitude':114.1521,\n",
    "            })\n",
    "            p_s_mag_prediction.update({epochs_loop+1:sub_p_s_mag_prediction})\n",
    "            break\n",
    "            \n",
    "        epochs_loop+=1\n",
    "        epochs+=1\n",
    "        start_+=mult\n",
    "        end_+=mult\n",
    "        time.sleep(1.25)\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "#create an instance of Flask\n",
    "app = flask.Flask('Earthquake Model Deployment')\n",
    "app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True\n",
    "\n",
    "@app.route('/')\n",
    "def home(): \n",
    "    return flask.jsonify(p_s_mag_prediction)\n",
    "\n",
    "app.run(debug=False, port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0f445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b7acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6118d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08257b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
